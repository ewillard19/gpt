{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from positional_encoding import PositionalEncoding\n",
    "from my_embedding import MyEmbedding\n",
    "from transformer import TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daily_dialog\")\n",
    "train_dialogs = dataset[\"train\"][\"dialog\"]\n",
    "val_dialogs   = dataset[\"validation\"][\"dialog\"]\n",
    "\n",
    "def concat_dialogs(dialogs, group_size=3):\n",
    "    corpus = []\n",
    "    for dialog in dialogs:\n",
    "        group = []\n",
    "        for s in dialog:\n",
    "            if s.strip():\n",
    "                group.append(s.strip())\n",
    "                if len(group) == group_size:\n",
    "                    corpus.append(\" \".join(group))\n",
    "                    group = []\n",
    "        if group:  # Ajoute le reste\n",
    "            corpus.append(\" \".join(group))\n",
    "    return corpus\n",
    "\n",
    "train_corpus = concat_dialogs(train_dialogs, group_size=3)\n",
    "val_corpus   = concat_dialogs(val_dialogs, group_size=3)\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "with open(\"train_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in train_corpus:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=\"train_corpus.txt\", vocab_size=6000, min_frequency=2, special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<PAD>\"), pad_token=\"<PAD>\")\n",
    "tokenizer.enable_truncation(max_length=64)\n",
    "\n",
    "def encode_batch(corpus, tokenizer):\n",
    "    encoded = []\n",
    "    for line in corpus:\n",
    "        ids = tokenizer.encode(f\"<BOS> {line} <EOS>\").ids\n",
    "        encoded.append(ids)\n",
    "    return encoded\n",
    "\n",
    "train_encoded = encode_batch(train_corpus, tokenizer)\n",
    "val_encoded   = encode_batch(val_corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, max_len=64):\n",
    "        super().__init__()\n",
    "        self.embed = MyEmbedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model) for _ in range(2)])\n",
    "        self.to_logits = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, pad_mask=None): \n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, pad_mask=pad_mask) \n",
    "        logits = self.to_logits(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, encoded_corpus):\n",
    "        self.data = encoded_corpus\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])\n",
    "        target_ids = torch.tensor(seq[1:])\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs  = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.token_to_id(\"<PAD>\"))\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=tokenizer.token_to_id(\"<PAD>\"))\n",
    "    # pad_mask: True là où c'est du padding\n",
    "    pad_mask = (inputs == tokenizer.token_to_id(\"<PAD>\"))\n",
    "    return inputs, targets, pad_mask\n",
    "\n",
    "train_dataset = LanguageDataset(train_encoded)\n",
    "val_dataset   = LanguageDataset(val_encoded)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader    = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 5.2167 | Val Loss: 4.4650\n",
      "Epoch 2 - Train Loss: 4.3550 | Val Loss: 4.1346\n",
      "Epoch 3 - Train Loss: 4.0997 | Val Loss: 3.9531\n",
      "Epoch 4 - Train Loss: 3.9274 | Val Loss: 3.8291\n",
      "Epoch 5 - Train Loss: 3.7982 | Val Loss: 3.7598\n",
      "Epoch 6 - Train Loss: 3.6963 | Val Loss: 3.6823\n",
      "Epoch 7 - Train Loss: 3.6146 | Val Loss: 3.6405\n",
      "Epoch 8 - Train Loss: 3.5446 | Val Loss: 3.6078\n",
      "Epoch 9 - Train Loss: 3.4873 | Val Loss: 3.5740\n",
      "Epoch 10 - Train Loss: 3.4370 | Val Loss: 3.5504\n",
      "Epoch 11 - Train Loss: 3.3913 | Val Loss: 3.5313\n",
      "Epoch 12 - Train Loss: 3.3518 | Val Loss: 3.5274\n",
      "Epoch 13 - Train Loss: 3.3158 | Val Loss: 3.5054\n",
      "Epoch 14 - Train Loss: 3.2850 | Val Loss: 3.4906\n",
      "Epoch 15 - Train Loss: 3.2557 | Val Loss: 3.4814\n",
      "Epoch 16 - Train Loss: 3.2280 | Val Loss: 3.4769\n",
      "Epoch 17 - Train Loss: 3.2033 | Val Loss: 3.4665\n",
      "Epoch 18 - Train Loss: 3.1810 | Val Loss: 3.4605\n",
      "Epoch 19 - Train Loss: 3.1590 | Val Loss: 3.4642\n",
      "Epoch 20 - Train Loss: 3.1397 | Val Loss: 3.4651\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = MiniTransformerLM(vocab_size=vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for input_ids, target_ids, pad_mask in train_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        pad_mask = pad_mask.to(device)\n",
    "\n",
    "        logits = model(input_ids, pad_mask=pad_mask)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = target_ids.view(-1)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids, pad_mask in val_loader:\n",
    "            input_ids = input_ids.to(device).long()\n",
    "            target_ids = target_ids.to(device).long()\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = target_ids.view(-1)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yesterday, he was a big town , and so he was . \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def generate_text(model, tokenizer, start, max_len=30, device=\"cpu\", temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(f\"<BOS> {start}\").ids\n",
    "    input_ids = torch.tensor([ids], device=device)\n",
    "    for _ in range(max_len):\n",
    "        logits = model(input_ids)\n",
    "        logits = logits[0, -1] / temperature\n",
    "        # Top-k sampling\n",
    "        topk = logits.topk(top_k)\n",
    "        probs = F.softmax(topk.values, dim=-1)\n",
    "        next_token_id = topk.indices[torch.multinomial(probs, 1).item()].item()\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "        if next_token_id == tokenizer.token_to_id(\"<EOS>\"):\n",
    "            break\n",
    "    ids = input_ids[0].tolist()\n",
    "    if ids[0] == tokenizer.token_to_id(\"<BOS>\"):\n",
    "        ids = ids[1:]\n",
    "    if tokenizer.token_to_id(\"<EOS>\") in ids:\n",
    "        ids = ids[:ids.index(tokenizer.token_to_id(\"<EOS>\"))]\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "\n",
    "print(generate_text(model, tokenizer, \"Yesterday, he\", device=device))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
