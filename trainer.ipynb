{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from positional_encoding import PositionalEncoding\n",
    "from my_embedding import MyEmbedding\n",
    "from transformer import TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS> say , jim , how about going for a few beers after dinner ? EOS', '<BOS> you know that is tempting but is really not good for our fitness . EOS', '<BOS> what do you mean ? it will help us to relax . EOS', \"<BOS> do you really think so ? i don't . it will just make us fat and act silly . remember last time ? EOS\", \"<BOS> i guess you are right.but what shall we do ? i don't feel like sitting at home . EOS\", '<BOS> i suggest a walk over to the gym where we can play singsong and meet some of our friends . EOS', \"<BOS> that's a good idea . i hear mary and sally often go there to play pingpong.perhaps we can make a foursome with them . EOS\", '<BOS> sounds great to me ! if they are willing , we could ask them to go dancing with us.that is excellent exercise and fun , too . EOS', \"<BOS> good.let ' s go now . EOS\", '<BOS> all right . EOS']\n",
      "['<BOS> good morning , sir . is there a bank near here ? EOS', '<BOS> there is one . 5 blocks away from here ? EOS', \"<BOS> well , that's too far.can you change some money for me ? EOS\", '<BOS> surely , of course . what kind of currency have you got ? EOS', '<BOS> rib . EOS', '<BOS> how much would you like to change ? EOS', '<BOS> 1000 yuan.here you are . EOS', '<BOS> good afternoon . this is michelle li speaking , calling on behalf of iba . is mr meng available at all ? EOS', '<BOS> this is mr meng speaking , michelle . EOS', \"<BOS> oh , hello ! sorry about that . i'm just calling to say that we've received your new corporate credit card from hq . EOS\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataset_ = load_dataset(\"daily_dialog\")\n",
    "texts = dataset_[\"train\"][\"dialog\"]\n",
    "vals = dataset_[\"validation\"][\"dialog\"]\n",
    "\n",
    "def get_corpus(texts):\n",
    "\n",
    "    corpus = []\n",
    "    for dialog in texts:\n",
    "        for sentence in dialog:\n",
    "            if sentence.strip():\n",
    "                corpus.append('<BOS> ' + sentence.strip() + ' <EOS>')\n",
    "    \n",
    "    print(corpus[:10])\n",
    "\n",
    "    def clean(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # supprime ponctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    corpus = [clean(s) for s in corpus if len(s.strip()) > 0]\n",
    "\n",
    "\n",
    "    tokens = set(\" \".join(corpus).split())\n",
    "    vocab = {word: i+1 for i, word in enumerate(tokens)}  # +1 pour réserver 0 = padding\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    inv_vocab = {i: w for w, i in vocab.items()}\n",
    "\n",
    "    encoded_corpus = []\n",
    "    for lines in corpus:\n",
    "        encoded_corpus.append([vocab[word] for word in lines.split()])\n",
    "    \n",
    "    return encoded_corpus, vocab, inv_vocab, tokens\n",
    "\n",
    "encoded_corpus, vocab, inv_vocab, tokens = get_corpus(texts)\n",
    "\n",
    "encoded_val, _, _, _ = get_corpus(vals)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a88a27ed35416a96fd94d6f514a248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbb6af3f44a45de8f75a1921457b130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ae85b577754afbb0b6a4ca11db7a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a429037104248b835872974d3552a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559d335feaa54438a310c24946704e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_ = load_dataset(\"daily_dialog\")\n",
    "texts = dataset_[\"train\"][\"dialog\"]\n",
    "\n",
    "def get_corpus(texts):\n",
    "    corpus = []\n",
    "    for dialog in texts:\n",
    "        for sentence in dialog:\n",
    "            if sentence.strip():\n",
    "                corpus.append('<BOS> ' + sentence.strip() + ' <EOS>')\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus(texts)\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'})\n",
    "\n",
    "encoded_corpus = [tokenizer.encode(s) for s in corpus]\n",
    "\n",
    "# Pour obtenir le vocabulaire :\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus = [line for line in encoded_corpus if len(line) <= 128]\n",
    "encoded_val = [line for line in encoded_val if len(line) <= 128]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embed = MyEmbedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model) for _ in range(2)])\n",
    "        self.to_logits = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        logits = self.to_logits(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, encoded_corpus):\n",
    "        self.data = encoded_corpus\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])   # tous sauf dernier\n",
    "        target_ids = torch.tensor(seq[1:])   # tous sauf premier\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LanguageDataset(encoded_corpus)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = LanguageDataset(encoded_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 4.2182 | Val Loss: 13.6262\n",
      "Epoch 2 - Train Loss: 3.6479 | Val Loss: 15.3556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m targets \u001b[38;5;241m=\u001b[39m target_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, targets)\n\u001b[0;32m---> 21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MiniTransformerLM(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for input_ids, target_ids in loader:\n",
    "        input_ids = input_ids.to(device).long()\n",
    "        target_ids = target_ids.to(device).long()\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = target_ids.view(-1)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device).long()\n",
    "            target_ids = target_ids.to(device).long()\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = target_ids.view(-1)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i you\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, vocab, inv_vocab, start, max_len=30, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    tokens = [vocab.get(word, vocab[\"<PAD>\"]) for word in start.split()]\n",
    "    tokens = torch.tensor(tokens, device=device).unsqueeze(0)\n",
    "    logits = model(tokens)\n",
    "    next_token = logits[0, -1].argmax().item()\n",
    "    tokens = torch.cat([tokens, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "    return \" \".join([inv_vocab[tok.item()] for tok in tokens[0]])\n",
    "\n",
    "# Exemple d’utilisation :\n",
    "print(generate_text(model, vocab, inv_vocab, \"i\", device=device))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
