{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from positional_encoding import PositionalEncoding\n",
    "from my_embedding import MyEmbedding\n",
    "from transformer import TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = load_dataset(\"daily_dialog\")\n",
    "texts = dataset_[\"train\"][\"dialog\"]\n",
    "vals = dataset_[\"validation\"][\"dialog\"]\n",
    "\n",
    "def get_corpus(texts):\n",
    "\n",
    "    corpus = []\n",
    "    for dialog in texts:\n",
    "        for sentence in dialog:\n",
    "            if sentence.strip():\n",
    "                corpus.append(sentence.strip().lower())\n",
    "\n",
    "    def clean(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # supprime ponctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    corpus = [clean(s) for s in corpus if len(s.strip()) > 0]\n",
    "\n",
    "\n",
    "    tokens = set(\" \".join(corpus).split())\n",
    "    vocab = {word: i+1 for i, word in enumerate(tokens)}  # +1 pour réserver 0 = padding\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    inv_vocab = {i: w for w, i in vocab.items()}\n",
    "\n",
    "    encoded_corpus = []\n",
    "    for lines in corpus:\n",
    "        encoded_corpus.append([vocab[word] for word in lines.split()])\n",
    "    \n",
    "    return encoded_corpus, vocab, inv_vocab, tokens\n",
    "\n",
    "encoded_corpus, vocab, inv_vocab, tokens = get_corpus(texts)\n",
    "\n",
    "encoded_val, _, _, _ = get_corpus(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus = [line for line in encoded_corpus if len(line) <= 30]\n",
    "encoded_val = [line for line in encoded_val if len(line) <= 30]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, max_len=30):\n",
    "        super().__init__()\n",
    "        self.embed = MyEmbedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model) for _ in range(2)])\n",
    "        self.to_logits = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        logits = self.to_logits(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, encoded_corpus):\n",
    "        self.data = encoded_corpus\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])   # tous sauf dernier\n",
    "        target_ids = torch.tensor(seq[1:])   # tous sauf premier\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LanguageDataset(encoded_corpus)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = LanguageDataset(encoded_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 5.5466 | Val Loss: 12.4816\n",
      "Epoch 2 - Train Loss: 4.6704 | Val Loss: 13.4510\n",
      "Epoch 3 - Train Loss: 4.3493 | Val Loss: 14.6148\n",
      "Epoch 4 - Train Loss: 4.1317 | Val Loss: 15.1319\n",
      "Epoch 5 - Train Loss: 3.9628 | Val Loss: 15.3893\n",
      "Epoch 6 - Train Loss: 3.8245 | Val Loss: 16.6777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     14\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m target_ids\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 16\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     18\u001b[0m targets \u001b[38;5;241m=\u001b[39m target_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 10\u001b[0m, in \u001b[0;36mMiniTransformerLM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(x)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MiniTransformerLM(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for input_ids, target_ids in loader:\n",
    "        input_ids = input_ids.to(device).long()\n",
    "        target_ids = target_ids.to(device).long()\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = target_ids.view(-1)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device).long()\n",
    "            target_ids = target_ids.to(device).long()\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = target_ids.view(-1)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i you\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, vocab, inv_vocab, start, max_len=30, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    tokens = [vocab.get(word, vocab[\"<PAD>\"]) for word in start.split()]\n",
    "    tokens = torch.tensor(tokens, device=device).unsqueeze(0)\n",
    "    logits = model(tokens)\n",
    "    next_token = logits[0, -1].argmax().item()\n",
    "    tokens = torch.cat([tokens, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "    return \" \".join([inv_vocab[tok.item()] for tok in tokens[0]])\n",
    "\n",
    "# Exemple d’utilisation :\n",
    "print(generate_text(model, vocab, inv_vocab, \"i\", device=device))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
